# CNN SÄ±nÄ±flandÄ±rma Projesi

## ğŸ“‹ Proje AÃ§Ä±klamasÄ±

Bu proje, kendi verisetimiz Ã¼zerinde Ã¼Ã§ farklÄ± CNN (Convolutional Neural Network) modeli geliÅŸtirerek gÃ¶rsel sÄ±nÄ±flandÄ±rma problemi Ã§Ã¶zmektedir. Projede Transfer Learning, Basit CNN ve Hiperparametre Optimizasyonu teknikleri uygulanmÄ±ÅŸtÄ±r.

---

## ğŸ“ Proje YapÄ±sÄ±

```
Proje1_CNN/
â”œâ”€â”€ README.md                 # Proje aÃ§Ä±klamasÄ±
â”œâ”€â”€ model1.ipynb             # Transfer Learning (VGG16)
â”œâ”€â”€ model2.ipynb             # Basit CNN Mimarisi
â”œâ”€â”€ model3.ipynb             # Hiperparametre Optimizasyonu
â””â”€â”€ dataset/
    â”œâ”€â”€ sinif1/              # SÄ±nÄ±f 1 gÃ¶rÃ¼ntÃ¼leri
    â””â”€â”€ sinif2/              # SÄ±nÄ±f 2 gÃ¶rÃ¼ntÃ¼leri
```

---

## ğŸ¯ Proje Hedefleri

1. **Model1:** Transfer Learning kullanarak veri Ã§eÅŸitliliÄŸi az olsa bile yÃ¼ksek doÄŸruluk elde etmek
2. **Model2:** SÄ±fÄ±rdan bir CNN oluÅŸturarak basit bir mimari ile Ã¶ÄŸrenmeyi anlamak
3. **Model3:** Hiperparametreleri optimize edererek model performansÄ±nÄ± artÄ±rmak

---

## ğŸ“Š Model AÃ§Ä±klamalarÄ±

### Model 1: Transfer Learning (VGG16)

**Teknik:** Transfer Learning + Fine-tuning

**Mimari:**
```
ImageNet eÄŸitilmiÅŸ VGG16 (donmuÅŸ katmanlar)
        â†“
    Flatten (128Ã—128Ã—3 â†’ vektor)
        â†“
    Dense(256, ReLU)
        â†“
    Dropout(0.5)
        â†“
    Dense(num_classes, Softmax)
```

**AvantajlarÄ±:**
- ImageNet'teki genel Ã¶zellikleri kullanÄ±r
- Az veriyle hÄ±zlÄ± eÄŸitim
- Genellikle en yÃ¼ksek doÄŸruluk

**Parametreler:**
- Epochs: 10
- Batch Size: 32
- Optimizer: Adam
- Loss: Categorical Crossentropy

---

### Model 2: Basit CNN

**Teknik:** SÄ±fÄ±rdan CNN mimarisi (no transfer learning)

**Mimari:**
```
Input(128Ã—128Ã—3)
    â†“
Conv2D(32) â†’ MaxPool â†’ Dropout(0.25)
    â†“
Conv2D(64) â†’ MaxPool â†’ Dropout(0.25)
    â†“
Conv2D(128) â†’ MaxPool â†’ Dropout(0.25)
    â†“
Flatten â†’ Dense(512, ReLU) â†’ Dropout(0.5) â†’ Output
```

**Ã–zellikler:**
- TÃ¼m katmanlar sÄ±fÄ±rdan eÄŸitilir
- Verisetine Ã¶zel Ã¶zellikler Ã¶ÄŸrenir
- Daha fazla eÄŸitim sÃ¼resi gerektirebilir

**Parametreler:**
- Epochs: 20
- Batch Size: 32
- Optimizer: Adam
- Loss: Categorical Crossentropy

---

### Model 3: Hiperparametre Optimizasyonu

**Teknik:** FarklÄ± konfigÃ¼rasyonlarla 6 deney

**Deneyler:**
1. **Baseline:** Model2 konfigÃ¼rasyonu
2. **Batch Size ArtÄ±ÅŸÄ±:** 32 â†’ 64
3. **Learning Rate AzalmasÄ±:** 0.001 â†’ 0.0005
4. **Dropout ArtÄ±ÅŸÄ±:** 0.25 â†’ 0.4
5. **Data Augmentation:** Rotation, Shift, Flip
6. **Kombinasyon:** Batchâ†‘ + Dropoutâ†‘ + Aug

**Data Augmentation Parametreleri:**
```python
rotation_range=15        # Â±15Â° dÃ¶ndÃ¼rme
width_shift_range=0.1    # Â±10% yatay kaydÄ±rma
height_shift_range=0.1   # Â±10% dikey kaydÄ±rma
horizontal_flip=True     # Yatay Ã§evirme
```

**Beklentiler:**
- Data augmentation aÅŸÄ±rÄ± uydurmayÄ± azaltÄ±r
- Batch size deÄŸiÅŸimi stabiliteyi etkileyebilir
- Dropout artÄ±ÅŸÄ± genelleÅŸtirmeyi iyileÅŸtirebilir
- Kombinasyon en iyi sonuÃ§larÄ± verebilir

---

## ğŸ“ˆ Beklenen SonuÃ§lar

### Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Aspect | Model1 | Model2 | Model3 |
|--------|--------|--------|--------|
| **HÄ±z** | HÄ±zlÄ± | YavaÅŸ | Orta-HÄ±zlÄ± |
| **DoÄŸruluk** | YÃ¼ksek | Orta | DeÄŸiÅŸken |
| **Veri TalebÄ±** | DÃ¼ÅŸÃ¼k | YÃ¼ksek | Orta |
| **EÄŸitim SÃ¼resi** | KÄ±sa | Uzun | Orta |

**Genel Tahmin:**
- **Model1** genellikle en yÃ¼ksek doÄŸruluk
- **Model3** verisetine baÄŸlÄ± olarak Model1'i geÃ§ebilir
- **Model2** baseline olarak iÅŸlev gÃ¶rÃ¼r

---

## ğŸ“¸ Veri Seti HazÄ±rlÄ±ÄŸÄ±

### Gereksinimler:
- **SÄ±nÄ±f SayÄ±sÄ±:** 2 (en az)
- **GÃ¶rsel SayÄ±sÄ±:** Her sÄ±nÄ±f iÃ§in en az 50 (toplam 100+)
- **GÃ¶rsel Boyutu:** Minimum 64Ã—64, tercih edilen 128Ã—128
- **Ã‡eÅŸitlilik:** FarklÄ± aÃ§Ä±, Ä±ÅŸÄ±k, arka plan

### Dataset YapÄ±sÄ±:
```
dataset/
â”œâ”€â”€ sinif1/
â”‚   â”œâ”€â”€ img001.jpg
â”‚   â”œâ”€â”€ img002.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ sinif2/
    â”œâ”€â”€ img001.jpg
    â”œâ”€â”€ img002.jpg
    â””â”€â”€ ...
```

### Veri Ã–niÅŸleme:
- Rescaling: 1/255 (0-1 aralÄ±ÄŸÄ±na normalize)
- Validation Split: 20%
- Image Size: 128Ã—128 (tÃ¼m gÃ¶rseller)

---

## ğŸš€ Ã‡alÄ±ÅŸtÄ±rma

### Gerekli KÃ¼tÃ¼phaneler:
```bash
pip install tensorflow keras matplotlib pandas numpy
```

### Ã‡alÄ±ÅŸtÄ±rma AdÄ±mlarÄ±:

1. **FotolarÄ± Ekleyin:**
   ```
   dataset/sinif1/ ve dataset/sinif2/ klasÃ¶rlerine fotolarÄ± kopyalayÄ±n
   ```

2. **Model1 Ã‡alÄ±ÅŸtÄ±rÄ±n:**
   ```
   Jupyter Notebook'de model1.ipynb â†’ Run All
   ```

3. **Model2 Ã‡alÄ±ÅŸtÄ±rÄ±n:**
   ```
   Jupyter Notebook'de model2.ipynb â†’ Run All
   ```

4. **Model3 Ã‡alÄ±ÅŸtÄ±rÄ±n:**
   ```
   Jupyter Notebook'de model3.ipynb â†’ Run All
   Grafikler ve karÅŸÄ±laÅŸtÄ±rma tablosunu inceleyiniz
   ```

---

## ğŸ“ Ã–nemli Notlar

### Transfer Learning vs. SÄ±fÄ±rdan EÄŸitim
- **Transfer Learning (Model1):** ImageNet'teki genel Ã¶zelliklerden yararlanÄ±r
- **SÄ±fÄ±rdan EÄŸitim (Model2):** Verisetine Ã¶zel Ã¶zellikler Ã¶ÄŸrenir

### AÅŸÄ±rÄ± UydurmayÄ± (Overfitting) Azaltma Teknikleri
1. **Dropout:** Rastgele nÃ¶ronlarÄ± devre dÄ±ÅŸÄ± bÄ±rakma
2. **Data Augmentation:** EÄŸitim setini Ã§eÅŸitlendirme
3. **Batch Normalization:** (opsiyonel) Katman Ã§Ä±ktÄ±larÄ±nÄ± normalize etme
4. **Early Stopping:** (opsiyonel) DoÄŸrulama doÄŸruluÄŸu dÃ¼ÅŸtÃ¼ÄŸÃ¼nde durdurma

### Hiperparametre Tuning
```
Best Practices:
- Bir parametre bir seferde deÄŸiÅŸtir
- Her deneyden sonra sonuÃ§larÄ± kaydet
- Deneylerin sayÄ±sÄ± Ã§ok olsa iyi olur (5-10+)
- Random seed ayarla tekrarlanabilirlik iÃ§in
```

---

## ğŸ“Š SonuÃ§ Analizi

Modeller eÄŸitildikten sonra:

1. **Grafikleri Ä°nceleyiniz:**
   - DoÄŸruluk ve kayÄ±p eÄŸrileri
   - EÄŸitim vs. doÄŸrulama farkÄ±
   - AÅŸÄ±rÄ± uydurmayÄ± kontrol et

2. **KarÅŸÄ±laÅŸtÄ±rma Tablosunu Analiz Edin:**
   - Hangi hiperparametre en etkili?
   - Model3 vs. Model1 farkÄ± nedir?
   - Neden bÃ¶yle sonuÃ§lar Ã§Ä±ktÄ±?

3. **Sorulara Cevap Verin:**
   - Transfer Learning neden avantajlÄ±?
   - Data Augmentation etkisi nedir?
   - Batch size artÄ±ÅŸÄ± neden Ã¶nemli?

---

## ğŸ“ EÄŸitim Ã‡Ä±karÄ±mlarÄ±

Bu proje sayesinde ÅŸunlarÄ± Ã¶ÄŸreneceksiniz:

âœ“ CNN mimarilerinin temel prensipleri  
âœ“ Transfer Learning ve fine-tuning  
âœ“ Hiperparametre optimizasyonu  
âœ“ Data augmentation teknikleri  
âœ“ Model karÅŸÄ±laÅŸtÄ±rmasÄ± ve analizi  
âœ“ Overftting / Underfitting anlamÄ±  

---

## ğŸ” Gelecek Ä°yileÅŸtirmeler

Proje geliÅŸtirildikÃ§e ÅŸunlar yapÄ±labilir:

1. **Daha Ä°yi Mimariler:** ResNet50, EfficientNet, Inception
2. **Learning Rate Scheduling:** Epoch'lar arttÄ±kÃ§a learning rate azalt
3. **Early Stopping:** DoÄŸrulama doÄŸruluÄŸu dÃ¼ÅŸtÃ¼ÄŸÃ¼nde durdur
4. **Class Weights:** Dengesiz verisetleri iÃ§in aÄŸÄ±rlÄ±k atama
5. **K-Fold Cross Validation:** Daha gÃ¼venilir sonuÃ§lar iÃ§in
6. **Model Ensemble:** Birden fazla modeli birleÅŸtir

---

## ğŸ“š Kaynaklar

- [Keras Documentation](https://keras.io/)
- [TensorFlow Transfer Learning](https://www.tensorflow.org/tutorials/images/transfer_learning)
- [VGG Paper](https://arxiv.org/abs/1409.1556)
- [CNN Visualization](https://distill.pub/2019/computing-receptive-fields/)

---

## âœï¸ Yazar Bilgileri

**AdÄ±:** [ADINI YAZ]  
**SoyadÄ±:** [SOYADINI YAZ]  
**Okul NumarasÄ±:** [NUMARANI YAZ]  
**GitHub:** https://github.com/kullanici_adi/CNN_siniflandirma

---

## ğŸ“„ Lisans

Bu proje eÄŸitim amaÃ§lÄ± olarak hazÄ±rlanmÄ±ÅŸtÄ±r.

---

**Son GÃ¼ncelleme:** AralÄ±k 2025
